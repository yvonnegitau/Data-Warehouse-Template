{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Overview","text":"<p>This is a project that will experiement the different aspects of the model data stack. We will start from the basic and slowly build up the platform by testing different tools, architectures and methodologies. At the end, we will have a platform that collects women football data from different leagues and makes it accessible for other to analyse and create interesting stories with the data. The tools used will all be open source.</p>"},{"location":"#architecture","title":"Architecture","text":"<p>The platform will that the following layers:</p> <ul> <li>Data Extraction Layer</li> <li>Data Catalog Layer</li> <li>Data Storage Layer</li> <li>Data Transformation and Modelling Layer</li> <li>Data Serving Layer</li> <li>Data Orchestration Layer</li> <li>Data Monitoring Layer</li> </ul> <p>Below is a base design that we will start with then we will build it up as we go.</p> <p></p>"},{"location":"#project-layout","title":"Project layout","text":"<pre><code>mkdocs.yml    # The configuration file.\ndocs/\n    index.md  # The documentation homepage.\n    ...       # Other markdown pages, images and other files.\n</code></pre>"},{"location":"monitoring/","title":"Data Warehouse Monitoring","text":"<p>We will use Prometheus to collect metrics from the different services and we will use grafana for displaying the metrics.</p>"},{"location":"monitoring/#dagster","title":"Dagster","text":"<p>For monitoring Dagster pipelines, we will first install <code>dagster-prometheus</code></p> <p><code>pip install dagster-prometheus</code></p> <p>Then we will add prometheus to the <code>workspace.yaml</code> file</p> <pre><code>resources:\n  prometheus:\n    config:\n      push_gateway_url: 'http://localhost:9091/'\n</code></pre> <p>Add the scrape job on <code>prometheus.yml</code></p> <pre><code>  - job_name: prometheus\n    honor_timestamps: true\n    scrape_interval: 15s\n    scrape_timeout: 10s\n    metrics_path: /metrics\n    scheme: http\n    static_configs:\n    - targets:\n      - localhost:9090\n\n</code></pre>"},{"location":"monitoring/#minio","title":"Minio","text":"<p>We will add a test json file for the minio dashboard. We also need to add the scrape job on <code>prometheus.yml</code> so that it can collect the metrics</p> <pre><code>- job_name: minio-job\n  metrics_path: /minio/prometheus/metrics\n  scheme: http\n  static_configs:\n      - targets: ['minio:9000']\n</code></pre>"},{"location":"orchestration/","title":"Data Orchestration","text":"<p>For data orchestration we will be using Dagster. We will add Dagster to our docker compose. When running Dagster, one needs the below long running containers :</p> <ul> <li>Webserver</li> <li>Daemon</li> <li>Code Location (each code has its own container)</li> <li>Metadata database</li> </ul> <p>Each run will be executed on its own container. Webserver and Daemon will have the same Docker image. We will also have files below which are a requirement:</p> <ul> <li><code>workspace.yaml</code> tells the webserver and daemon the location of the code</li> <li><code>dagster.yaml</code> configures the dagster instance</li> </ul>"},{"location":"orchestration/#reverse-proxy","title":"Reverse Proxy","text":"<p>Dagster OSS does not come with authentication. I have added basic authentication to secure the webserver.</p> <p>NOTE: Sessions is not handled well. A better authentication should be used prod. This auth also does not handle RBAC. This only secures this setup.</p>"},{"location":"orchestration/#run-code","title":"Run Code","text":"<p><code>docker compose up</code></p> <p>The webserver can be accessed through <code>localhost:3000</code></p>"},{"location":"storage/","title":"Data Lake","text":"<p>We will use an object storage as the data lake. This will enable us to have both structured and unstructured data. In addition to this, object storage is cheaper compared to the other storage solutions.</p> <p>On local deployment, we will use Minio and delpoy it on docker. We will start with a single-node deployment then when we advance, we will also explore a multi-node deployment.</p>"},{"location":"storage/#deploying-minio","title":"Deploying Minio","text":""},{"location":"storage/#step-1-install-docker-and-docker-compose","title":"Step 1: Install Docker and Docker Compose","text":"<p>Install docker desktop following instructions on this link To confirm docker is installed, run <code>docker --version</code> To confirm docker-compose is installed, run <code>docker-compose --version</code></p>"},{"location":"storage/#step-2-create-docker-composeyml-file","title":"Step 2: Create <code>docker-compose.yml</code> file","text":"<p>For this project, we will use one docker-compose.yml file for all services.</p>"},{"location":"storage/#step-3-add-minio-service-to-docker-composeyml-file","title":"Step 3: Add Minio Service to <code>docker-compose.yml</code> file","text":"<p>A sample of the service details is in the <code>docker-compose.yml</code> file. The enviornment variables are stored in the .env file. You can find a template in <code>.env.template</code>. Make sure to change the variable details.</p>"},{"location":"storage/#step-4-run-docker-compose","title":"Step 4: Run docker-compose","text":"<p>Run <code>docker-compose up -d</code></p>"},{"location":"storage/#step-5-access-minio-server","title":"Step 5: Access Minio Server","text":"<p>To access the webserver, use (127.0.0.1:9001). The username is the value used for <code>MINIO_ROOT_USER</code> and Passowrd the value used for <code>MINI_ROOT_PASSWORD</code></p> <p>When you login, you should see this page: </p>"},{"location":"storage/#step-6-create-bucket","title":"Step 6: Create Bucket","text":"<p>The data that will be ingested will be stored in this bucket. The user to be created will be assigned policies that will be specific to this bucket.</p>"},{"location":"storage/#step-7-create-user","title":"Step 7: Create User","text":"<p>This user will be used by applications to insert data to minio. This is more secure that using the Root User. In the navigation bar identity -&gt; User</p>"},{"location":"storage/#step-8-create-policy","title":"Step 8: Create Policy","text":"<p>We will create a policy that the user will be assigned. We would like the user to be able to add, delete and retrieve objects in the bucket. This is a sample of a policy</p> <pre><code>{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"s3:ListBucket\",\n                \"s3:PutObject\",\n                \"s3:CreateBucket\",\n                \"s3:DeleteObject\",\n                \"s3:GetBucketLocation\",\n                \"s3:GetObject\",\n                \"s3:ListAllMyBuckets\"\n            ],\n            \"Resource\": [\n                \"arn:aws:s3:::lake*\"\n            ]\n        }\n    ]\n}\n</code></pre>"},{"location":"storage/#step-9-assign-user-the-policy","title":"Step 9: Assign User the policy","text":"<p>Under the identity -&gt; User, select the user that you have created. Under policies, there is a button named Assign Policies. Click on it and the policy that you created will be listed. Select it and save.</p> <p></p>"},{"location":"storage/#step-10-create-access-key-for-the-user-created","title":"Step 10: Create Access Key for the User created","text":"<p>Still under User, you will see service account. Click on it and you will see access key and password generated. Copy them and save them somewhere safe since you will you it later.</p> <p>Now Minio is ready to receive data.</p>"},{"location":"transformation_modelling/","title":"Transformation and Modelling","text":"<p>We will use dbt for any processig and modelling that will be done to the data. When we curate the data, we will move the curated to an analytical database.</p>"},{"location":"transformation_modelling/#clickhouse","title":"Clickhouse","text":"<p>We will be using clickhouse for the Analytics database. What I have learned is that the OSS does not have an in-built UI but you can use third-party app for this.</p> <p>TODO: Fine-tuning Clickhouse for larger datasets</p>"},{"location":"transformation_modelling/#duckdb","title":"DuckDB","text":"<p>We will use Duckdb and Iceberg connected to S3. This is an experiment that can change after the first project trial.</p>"}]}